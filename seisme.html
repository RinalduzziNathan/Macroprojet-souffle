<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>DÃ©tection de rapprochement des mains</title>

  <!-- External Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
  <script src="https://unpkg.com/@rive-app/canvas"></script>

  <style>
    body {
      margin: 0;
      overflow: hidden;
      background-image: url('interface/quadrillage.png');
      background-size: cover;
      background-position: center;
      background-repeat: no-repeat;
      height: 100vh;
      transition: background 0.5s;
    }

    canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%; 
      height: auto;
    }

    #riveCanvas {
        width: 1366;
        height: 768;
      z-index: 1;
    }

    #handCanvas {
      z-index: 2;
      pointer-events: none;
    }

    #video {
      display: none;
    }

    #clickZonePompe {
      position: absolute;
      top: 5px;
      left: 5px;
      width: 150px;
      height: 100px;
      cursor: pointer;
      background-color: rgba(255, 255, 255, 0);
      z-index: 3;
    }

    #expandingCircle {
      position: absolute;
      top: 0;
      left: 0;
      width: 250px;
      height: 250px;
      border-radius: 50%;
      background-color: white;
      border: 2.5px solid black;
      z-index: 1;
      transition: all 1s ease;
    }

    #pompeImage {
      position: absolute;
      top: 30px;
      left: 75px;
      z-index: 10;
      width: 90px;
      height: 205px;
      rotate: 10deg;
    }
  </style>
</head>
<body>
  <video id="video" autoplay playsinline></video>
  <canvas id="riveCanvas"></canvas>
  <canvas id="handCanvas"></canvas>

  <div id="clickZonePompe">
    <img id="pompeImage" src="interface/pompe.png" alt="Pompe" />
    <div id="expandingCircle"></div>
  </div>

  <script>
    async function startGestureRecognition() {
      const video = document.getElementById("video");
      const handCanvas = document.getElementById("handCanvas");
      const ctx = handCanvas.getContext("2d");

      let baseToVib, baseToOK, baseToPasOK, toBase;

      // Rive setup on separate canvas
      const r = new rive.Rive({
        src: "https://rinalduzzinathan.github.io/file-stash/rive/seisme.riv",
        canvas: document.getElementById("riveCanvas"),
        autoplay: true,
        stateMachines: "State Machine 1",
        onLoad: () => {
          const inputs = r.stateMachineInputs("State Machine 1");
          baseToVib = inputs.find(i => i.name === "baseToVib");
          baseToOK = inputs.find(i => i.name === "baseToOK");
          baseToPasOK = inputs.find(i => i.name === "baseToPasOK");
          toBase = inputs.find(i => i.name === "toBase");

          r.resizeDrawingSurfaceToCanvas();
        }
      });

      await tf.setBackend("webgl");
      console.log("Backend actif :", tf.getBackend());

      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;

      video.onloadedmetadata = () => {
        video.play();
        handCanvas.width = video.videoWidth;
        handCanvas.height = video.videoHeight;
        detectHands();
      };

      const hands = new Hands({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}` });
      hands.setOptions({
        maxNumHands: 2,
        modelComplexity: 1,
        minDetectionConfidence: 0.9,
        minTrackingConfidence: 0.9
      });

      hands.onResults((results) => {
        ctx.clearRect(0, 0, handCanvas.width, handCanvas.height);

        if (!results.multiHandLandmarks || results.multiHandLandmarks.length === 0) {
            if (toBase) toBase.fire(); // ðŸ‘ˆ Fire 'toBase' when no hands are seen
            return; // Exit early
        }

        if (results.multiHandLandmarks.length === 2) {
          const hand1 = results.multiHandLandmarks[0];
          const hand2 = results.multiHandLandmarks[1];

          drawHand(hand1);
          drawHand(hand2);

          if (areHandsTouching(hand1, hand2)) {
            if (baseToVib) baseToVib.fire();
          } else {
            if (toBase) toBase.fire();
          }
        }
      });

      async function detectHands() {
        await hands.send({ image: video });
        requestAnimationFrame(detectHands);
      }

      function drawHand(landmarks) {
        ctx.strokeStyle = "red";
        ctx.fillStyle = "red";
        ctx.lineWidth = 2;
        for (let i = 0; i < landmarks.length; i++) {
          const { x, y } = landmarks[i];
          ctx.beginPath();
          ctx.arc(x * handCanvas.width, y * handCanvas.height, 5, 0, 2 * Math.PI);
          ctx.fill();
        }
      }

      function areHandsTouching(hand1, hand2) {
        const index1 = hand1[8];
        const index2 = hand2[8];
        const distance = Math.sqrt(
          Math.pow((index1.x - index2.x) * handCanvas.width, 2) +
          Math.pow((index1.y - index2.y) * handCanvas.height, 2)
        );
        return distance < 90;
      }
    }

    startGestureRecognition();

    const pompeImage = document.getElementById('pompeImage');
    const expandingCircle = document.getElementById('expandingCircle');
    let isCircleExpanded = false;

    pompeImage.addEventListener('click', () => {
      pompeImage.style.display = 'none';
      expandingCircle.style.width = '650px';
      expandingCircle.style.height = '650px';
      isCircleExpanded = true;
    });

    expandingCircle.addEventListener('click', () => {
      if (isCircleExpanded) {
        expandingCircle.style.width = '250px';
        expandingCircle.style.height = '250px';
        pompeImage.style.display = 'block';
        isCircleExpanded = false;
      }
    });
  </script>
</body>
</html>
